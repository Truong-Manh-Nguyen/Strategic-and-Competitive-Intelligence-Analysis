# -*- coding: utf-8 -*-
"""TruongNguyen_Scientific_Publication_Analysis_with_BERTopic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gpu1Wbqdr_vCDMKC1OZIkeajhpFvi2vD

```
# Questo è formattato come codice
```

# **Scientific Publication Analysis with BERTopic**

BERTopic is a topic modeling technique that leverages transformers and a custom class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. (https://maartengr.github.io/BERTopic/index.html)

## Information about the algorihm
website: https://maartengr.github.io/BERTopic/algorithm/algorithm.html

paper: https://arxiv.org/pdf/2203.05794.pdf  

<img src="https://maartengr.github.io/BERTopic/img/algorithm.png" width="50%">

# Enabling the GPU

First, you'll need to enable GPUs for the notebook:

*   Navigate to Edit→Notebook Settings
*   Select GPU from the Hardware Accelerator drop-down

[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)

# **Installing BERTopic**

We start by installing BERTopic from PyPi:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install bertopic
# !pip install joblib==1.1.0

"""After installing BERTopic, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook. **From the Menu: Runtime → Restart Runtime**

There is an issue in the library update (not solved at the time of the analysis). See https://github.com/scikit-learn-contrib/hdbscan/issues/565

# **Import Data**
Import the dataset for the Topic Modelling. In this case we will analyse the scientific Publications about attrition. The dataset includes Title, Abstract, Authors Keywords, Year, Number of Citations and Authors of papers available on Scopus. The papers have been checked manually to select only the ones in scope for the purpose of the analysis.
"""

pip install google.colab

# Connect Google Drive (GDrive) with Colab
from google.colab import drive
drive.mount("/content/gdrive")

# @title Default title text
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Đọc tệp CSV
file_path = '/content/drive/MyDrive/Truong Nguyen/input/scopus.csv'
df = pd.read_csv(file_path)

# Hiển thị dữ liệu
print(df.head())



# Import the file from Google Drive
import pandas as pd
attrition_paper = pd.read_csv('/content/drive/MyDrive/Truong Nguyen/input/scopus.csv')

"""Let's have a look into the dataset and in the string that we will use for the topic modelling."""

attrition_paper.head()

"""# **Text Preprocessing**

1. Merge text from Title, Author Keywords, and Abstracts.
"""

attrition_paper[['Title', 'Abstract', 'Author Keywords']] = attrition_paper[['Title', 'Abstract', 'Author Keywords']].fillna('')
attrition_paper['Cited by'] = attrition_paper['Cited by'].fillna(0)
attrition_paper['text'] = attrition_paper['Title'] + ' ' + attrition_paper['Abstract'] + ' ' + attrition_paper['Author Keywords']

attrition_paper.head()

"""2. Lemmatize and clean text removing stopwords, scientific literature blacklist, and domain blacklist."""

# Import blacklist
# The literature Blacklist include a list of common terms from scientific literature
literature_blacklist = pd.read_csv('/content/drive/MyDrive/Truong Nguyen/dictionaries/literature_blacklist.csv')
# The Domain Blacklist includes the terms used in the keywords and the most frequent and the most rare terms from the abstracts in the dataset
domain_blacklist = pd.read_csv('/content/drive/MyDrive/Truong Nguyen/dictionaries/domain_blacklist.csv')

# Merge blacklist
blacklist = pd.concat([literature_blacklist, domain_blacklist])
len(blacklist)

# Remove duplicates (if any)
blacklist.drop_duplicates(subset='value', inplace=True)
len(blacklist)

# Transform blacklist in a list with .tolist()
blacklist_list = blacklist["value"].tolist()

# Configure cleaning operations
config = {
    'remove_punct' : True,
    'remove_num' : True,
    'remove_stopwords' : True,
    'lemmatize' : True,
    'remove_blacklist' : blacklist_list
}

# Define preprocessing funcion
import spacy

nlp = spacy.load('en_core_web_sm') # load language model

def preprocess_txt(text):
    text = text.lower() # convert to lower case
    doc = nlp(text) # apply language model
    if config['remove_punct']:
        doc = [token for token in doc if not token.is_punct]
    if config['remove_num']:
        doc = [token for token in doc if not token.is_digit]
    if config['remove_stopwords']:
        doc = [token for token in doc if not token.is_stop and token.text not in config['remove_blacklist']]
    if config['lemmatize']:
        doc = [token.lemma_ for token in doc]   # .lemma_ is a string
    if config['remove_blacklist']:
        doc = [token for token in doc if token not in config['remove_blacklist']]

    result = ''
    for text in doc:
        result += text + ' '

    return result.strip()

# Apply preprocessing funcion to text [PAY ATTENION: LONG]
attrition_paper['text_preprocessed'] = attrition_paper['text'].apply(lambda text: preprocess_txt(text))

attrition_paper.head()

# Save results
attrition_paper.to_csv(r'/content/drive/MyDrive/Truong Nguyen/wip/BERTopic_cleaned.csv', index = False, header=True)

"""# **Application of the BERTopic model**

Let's apply BERTopic using the techniques for imporving topic representation (with reference to the elimination of stopwords in defining the names of the clusters). We will customize UMAP only to set the random state to ensure reproducibility.

Then we will visualize the Topics' Hierarchy to get information on the structure of the clustering and the UMAP model to assess the clustering. The second is a visualization of the distribution of the embeddings in the clusters in a two-dimensional space, where each paper as a point, colored as the belonging cluster. The graph will provide the title of the papers moving on the graph.  

We will apply this pipeline both to raw and clean text.
"""

# Connect Google Drive (GDrive) with Colab [not needed if it is the same run]
from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
attrition_paper = pd.read_csv('/content/drive/MyDrive/Truong Nguyen/wip/BERTopic_cleaned.csv')

# Import pre-processed data [not needed if it is the same run]
import pandas as pd
attrition_paper = pd.read_csv(r'/content/drive/MyDrive/Truong Nguyen/wip/BERTopic_cleaned.csv')

"""TRY TO CHANGE THE **INITIAL CONDITION** (random_state in UMAP) AND CHECK HOW MUCH RESULTS DEPEND ON THE **INITIAL CONDITION**"""

# Set models
from scipy.cluster import hierarchy as sch
from bertopic import BERTopic
from umap import UMAP

# Set UMAP model
umap_model_new = UMAP(random_state=567)

# Set BERTopic model
topic_model_new = BERTopic(language="english", calculate_probabilities=True, verbose=True, n_gram_range =(1,2), umap_model=umap_model_new)

"""**Application to raw text**"""

# Apply model to raw text
topics_new_raw, probs_new_raw = topic_model_new.fit_transform(attrition_paper.text)
len(topic_model_new.get_topic_info())

freq_new_raw = topic_model_new.get_topic_info()
freq_new_raw

df_new_raw = pd.DataFrame({'Topic': topics_new_raw, 'scopus_id': attrition_paper.EID, 'year':attrition_paper.Year})
df_new_raw.head()

freq_new_raw

# Save results
freq_new_raw.to_csv (r'/content/drive/MyDrive/Truong Nguyen/output/BERTopics_new_raw_topic_freq2.csv', index = False, header=True)
df_new_raw.to_csv (r'/content/drive/MyDrive/Truong Nguyen/output/BERTopics_new_raw_paper2.csv', index = False, header=True)

from google.colab import drive
drive.flush_and_unmount()

from google.colab import drive
drive.mount('/content/drive')

# Hierarchical topics
hierarchical_topics_new_raw = topic_model_new.hierarchical_topics(attrition_paper.text)

# Visualize hierarchical topics in a tree
tree_new_raw = topic_model_new.get_topic_tree(hierarchical_topics_new_raw)
print(tree_new_raw)

# (copy and paste in a txt to save the result)

# Results from UMAP model

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from umap import UMAP

# Set UMAP model [note: ONLY random_stade ensures replication]
umap_model_new = UMAP(random_state=567)

# Prepare embeddings
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings_raw = sentence_model.encode(attrition_paper.text, show_progress_bar=False)

# Train BERTopic
topic_model_new2_raw = BERTopic(language="english", calculate_probabilities=True, verbose=True, n_gram_range =(1,2), umap_model=umap_model_new).fit(attrition_paper.text, embeddings_raw)

# Run the visualization with the original embeddings
topic_model_new2_raw.visualize_documents(attrition_paper.text, embeddings=embeddings_raw)

# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:
reduced_embeddings_raw = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings_raw)

# Set only numbers as labels (for a better visualization)
topic_labels = list((range(-1,2)))

for i in range(0,len(topic_labels)):
  topic_labels[i] = str(topic_labels[i])

topic_model_new2_raw.set_topic_labels(topic_labels)

# Visualize plot
fig_UMAP_new_raw = topic_model_new2_raw.visualize_documents(attrition_paper.Title, reduced_embeddings=reduced_embeddings_raw, hide_annotations = False, custom_labels= True, width = 800, height = 500)
fig_UMAP_new_raw

# Save results in html to have the interactive version
import plotly.express as px
fig_UMAP_new_raw.write_html("/content/drive/MyDrive/Truong Nguyen/output/BERTopics_new_raw_UMAP2.html", default_width = 1200, default_height = 1200)

"""**Application to clean text**"""

# Apply model to clean text
topics_new_clean, probs_new_clean = topic_model_new.fit_transform(attrition_paper.text_preprocessed)
len(topic_model_new.get_topic_info())

freq_new_clean = topic_model_new.get_topic_info()

freq_new_clean

df_new_clean = pd.DataFrame({'Topic': topics_new_clean, 'scopus_id': attrition_paper.EID, 'year':attrition_paper.Year})
df_new_clean.head()

# Save results
freq_new_clean.to_csv (r'/content/drive/MyDrive/Truong Nguyen/output/BERTopics_new_clean_topic_freq2(2).csv', index = False, header=True)
df_new_clean.to_csv (r'/content/drive/MyDrive/Truong Nguyen/output/BERTopics_new_clean_paper2(2).csv', index = False, header=True)

# Hierarchical topics
hierarchical_topics_new_clean = topic_model_new.hierarchical_topics(attrition_paper.text_preprocessed)

# Visualize hierarchical topics in a tree
tree_new_clean = topic_model_new.get_topic_tree(hierarchical_topics_new_clean)
print(tree_new_clean)

# (copy and paste in a txt to save the result)

# Visualize results from UMAP model

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from umap import UMAP
from bertopic.representation import MaximalMarginalRelevance


# Set UMAP model [note: ONLY random_stade ensures replication]
umap_model_new = UMAP(random_state=567)

# Prepare embeddings
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings_clean = sentence_model.encode(attrition_paper.text_preprocessed, show_progress_bar=False)


representation_model = MaximalMarginalRelevance(diversity=0.2)

# Train BERTopic

topic_model_new2_clean = BERTopic(
    language="english",
    calculate_probabilities=True,
    verbose=True,
    n_gram_range=(1, 2),
    umap_model=umap_model_new,
    representation_model=representation_model
).fit(attrition_paper.text_preprocessed, embeddings_clean)

# Run the visualization with the original embeddings
topic_model_new2_clean.visualize_documents(attrition_paper.text_preprocessed, embeddings=embeddings_clean)

# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:
reduced_embeddings_clean = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings_clean)
fig_UMAP_new_clean = topic_model_new2_clean.visualize_documents(attrition_paper.Title, reduced_embeddings=reduced_embeddings_clean)

# Set only numbers as labels (for a better visualization)
topic_labels_clean = list((range(-1,2)))

for i in range(0,len(topic_labels_clean)):
  topic_labels_clean[i] = str(topic_labels_clean[i])

topic_model_new2_clean.set_topic_labels(topic_labels_clean)

# Visualize plot
fig_UMAP_new_clean = topic_model_new2_clean.visualize_documents(attrition_paper.Title, reduced_embeddings=reduced_embeddings_clean, hide_annotations = False, custom_labels= True, width = 800, height = 500)
fig_UMAP_new_clean

# Save results in html to have the interactive version
import plotly.express as px
fig_UMAP_new_clean.write_html("/content/drive/MyDrive/Truong Nguyen/output/BERTopics_new_raw_UMAP2(2).html", default_width = 1200, default_height = 1200)

